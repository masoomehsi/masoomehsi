<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
    <meta name="description" content=""/>
    <meta name="author" content=""/>
    <title> PASS (Persian Audio Source Separation) </title>
<!--    <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico"/>-->
    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet"
          type="text/css"/>
    <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css"/>
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet"/>
</head>

    <body id="page-top">
<!-- Navigation-->
<nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none"> PASS (Persian Audio Source Separation) </span>
        <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-3"
                                             src="assets/img/logo.png" alt=""/></span>
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span
            class="navbar-toggler-icon"></span></button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Masoumehsiar">Speaker Count</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#FatemehJafari">Vocal-remover</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#mohammadi">Source separation</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Zargaran">Source separation</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Honors">Honors and Awards</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Projects">Publications</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">TECHNICAL SKILLS</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#interests">RESEARCH INTERESTS</a></li>
<!--            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>-->
        </ul> 
    </div>
</nav>
<!-- Page Content-->
<div class="container-fluid p-0">
    <!-- About-->
    <section class="resume-section" id="about">
        <div class="resume-section-content">
            <h1 class="mb-0">
                PASS
                <span class="text-primary">Project</span>
            </h1>
            <div class="lead mb-5">
                Persian Audio Source Separation 
   
            </div>
            
            <div class="lead mb-5">

هدف از پروژه جداسازي منبع صوت فارسي جداسازي دو يا چند فايل صوتي مجزا از يك صوت حاوي دو يا چند
منبع صوت است. فايل صوتي جلسه اي را كه در آن  ۴نفر هم زمان با هم صحبت مي كنند و يك موسيقي پس زمينه
هم در حال اجرا است را در نظر بگيريد، محصول نهايي اين پروژه بايد بتواند  ۵فايل صوتي مجزا خروجي دهد كه هر
كدام حاوي يكي صوت از يكي از اين منابع صوتي باشد ) ۴نفر و منبع موسيقي.( كتابخانه قرار داده شده در قسمت
رفرنس ها يك پروژه متن باز آماده براي اين كار است. ممكن است همين پروژه براي ما كافي باشد )ممكن است
وابسته به زبان نباشد(. در غير اين صورت مي بايست اقدامات لازم براي شخصي سازي آن براي زبان فارسي صورت
گيرد. پس از تحقيقات اوليه و ساخت يك دمو از زبان فارسي نوبت به تست محصول مي رسد در صورتي كه انتظارت
كيفي برآورده شود يك ميكروسرويس ارايه مي شود و دموي آن روي سايت شركت به عنوان يك محصول پژوهشي
در حوزه پردازش صوت قرار مي گيرد.
انتظار مي رود يك گزارش اوليه پس از تحقيقات انجام شده در وبلاگ شركت قرار گيرد و اجتماع اين گزارش و
گزارش هاي هفتگي نيز گزارش نهايي پروژه را بسازد
            
                        </div>

            
            <div class="lead mb-1" style="color: #000000">
                Address: Sharif University of Technology, Azadi Ave, Tehran, Iran

            </div>
            <div class="lead mb-1">

                <a href="info@asr-gooyesh.com" style="color: #000000">Email: info@asr-gooyesh.com</a>

            </div>
            <div class="lead mb-5" style="color: #000000">

                Phone Number: 021-66551525
            </div>
            <div class="social-icons">
                <a class="social-icon" href="https://github.com/agp-internship/pass"><i class="fab fa-github"></i></a>
                 <a class="social-icon" href="https://www.linkedin.com/company/asr-gooyesh-pardaz-co-/?originalSubdomain=ir"><i class="fab fa-linkedin"></i></a>
                 <a class="social-icon" href="https://www.instagram.com/asrgooyesh"><i class="fab fa-instagram"></i></a>
                 <a class="social-icon" href="http://telegram.me/asrgooyeshpardaz"><i class="fab fa-telegram"></i></a>

            </div>


        </div>
           
        </section>
    <hr class="m-0"/>
    
    <!-- Masoumeh siar-->
    <section class="resume-section" id="Masoumehsiar">
        <div class="resume-section-content">
            <h2 class="mb-5">Masoumeh siar</h2>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="./assets/img/masoumeh.png" style="width: 20%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="https://arxiv.org/pdf/1712.04555.pdf">Classification vs. Regression in Supervised Learning for Single Channel Speaker Count Estimation
                    </a></h3>
                    <div> In2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2018 Apr 15 (pp. 436-440). IEEE.</div>
                    <p class="mb-2">  </p>
                    <p> Abstract: The task of estimating the maximum number of concurrent speakers from single channel mixtures is important for various audio-based applications, such as blind
source separation, speaker diarisation, audio surveillance or auditory scene classification. Building upon powerful machine learning methodology, we develop a Deep Neural Network (DNN) that estimates a speaker count. While DNNs efficiently map input representations to output targets, it remains unclear how to best handle the network output to infer integer source count estimates, as a discrete
count estimate can either be tackled as a regression or a classification problem. In this paper, we investigate this important design decision and also address complementary parameter choices such as the input representation. We evaluate a stateof-the-art DNN audio model based on a Bi-directional Long Short-Term Memory network architecture for speaker count estimations. Through experimental evaluations aimed at identifying the best overall strategy for the task and show results for five seconds speech segments in mixtures of up to ten speakers.</p>
                    
                </div>
            </div>

         <div class="social-icons">
                <a class="social-icon" href="https://github.com/aishoot/Concurrent_Speakers_Counter"><i class="fab fa-github"></i></a>
                 <a class="social-icon" href="https://www.linkedin.com/company/asr-gooyesh-pardaz-co-/?originalSubdomain=ir"><i class="fab fa-home"></i></a>
                 <a class="social-icon" href=""><i class="fab fa-telegram"></i></a>

            </div>   
            
        </div>
    </section>
    
    
    
    
    
    
    
    
    
    
 
    
    <!-- Fatemeh Jafari-->
    <section class="resume-section" id="FatemehJafari">
        <div class="resume-section-content">
            <h2 class="mb-5">Fatemeh Jafari</h2>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="./assets/img/fatemeh.png" style="width: 20%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="https://arxiv.org/pdf/1805.02410.pdf">MMDENSELSTM: AN EFFICIENT COMBINATION OF CONVOLUTIONAL AND
RECURRENT NEURAL NETWORKS FOR AUDIO SOURCE SEPARATION

                    </a></h3>
                    <div>  In2018 16th International workshop on acoustic signal enhancement (IWAENC) 2018 Sep 17 (pp. 106-110). IEEE.</div>
                    <p class="mb-2">  </p>
                    <p> Abstract: Deep neural networks have become an indispensable technique for audio source separation (ASS). It was recently reported that a variant of CNN architecture called MMDenseNet was successfully employed to solve the ASS problem of estimating source amplitudes, and state-of-the-art results were obtained for DSD100 dataset. To further enhance
MMDenseNet, here we propose a novel architecture that integrates long short-term memory (LSTM) in multiple scales with skip connections to efficiently model long-term structures within an audio context. The experimental results show
that the proposed method outperforms MMDenseNet, LSTM and a blend of the two networks. The number of parameters and processing time of the proposed model are significantly
less than those for simple blending. Furthermore, the proposed method yields better results than those obtained using ideal binary masks for a singing voice separation task.</p>
                    
                </div>
            </div>

         <div class="social-icons">
                <a class="social-icon" href="https://github.com/tsurumeso/vocal-remover"><i class="fab fa-github"></i></a>
                 <a class="social-icon" href=""><i class="fab fa-telegram"></i></a>

            </div>   
            
        </div>
    </section>
    
    
    
    
    
    
    
    
        
            </section>
    <hr class="m-0"/>
    <!-- mohammadi-->
    <section class="resume-section" id="mohammadi">
        <div class="resume-section-content">
            <h2 class="mb-5">‪Sajjad Mohammadi and AmirAli Rezaei</h2>  
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="./assets/img/mohammadi.png" style="width: 20%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="https://arxiv.org/pdf/2005.04132.pdf"> Asteroid: the PyTorch-based audio source separation toolkit for researchers

                    </a></h3>
                    <div>  arXiv preprint arXiv:2005.04132. 2020 May 8. </div>
                    <p class="mb-2">  </p>
                    <p> Abstract: This paper describes Asteroid, the PyTorch-based audio source separation toolkit for researchers. Inspired by the most successful neural source separation systems, it provides all neural building blocks required to build such a system. To improve reproducibility, Kaldi-style recipes on common audio source separation datasets are also provided. This paper describes the software architecture of Asteroid and its most important features. By showing experimental results obtained with Asteroid’s recipes, we show that our implementations are at least on par with most results reported in reference papers. The
toolkit is publicly available at github.com/mpariente/asteroid.
                        
                </div>
            </div>

         <div class="social-icons">
                <a class="social-icon" href="https://github.com/asteroid-team/asteroid"><i class="fab fa-github"></i></a>
                 <a class="social-icon" href=""><i class="fab fa-telegram"></i></a>

            </div>   
            
        </div>
    </section>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
   
    <!-- ‪Alireza Zargaran-->
    <section class="resume-section" id="Zargaran">
        <div class="resume-section-content">
            <h2 class="mb-5">‪Alireza Zargaran</h2>  
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="./assets/img/mohammadi.png" style="width: 20%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="https://arxiv.org/pdf/1910.09804.pdf"> TWO-STEP SOUND SOURCE SEPARATION: TRAINING ON LEARNED LATENT TARGETS
                    </a></h3>
                    <div>     InICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2020 May 4 (pp. 31-35). IEEE.
 </div>
                    <p class="mb-2">  </p>
                    <p> Abstract: In this paper, we propose a two-step training procedure for source separation via a deep neural network. In the first step we learn a transform (and it’s inverse) to a latent space where masking-based separation performance using oracles is optimal. For the second step, we train a separation module that operates on the previously learned space. In order to do so, we also make use of a scale-invariant signal to distortion ratio (SI-SDR) loss function that works in the latent space, and we prove that it lower-bounds the SI-SDR in the time domain. We run various sound separation experiments that show how
this approach can obtain better performance as compared to systems that learn the transform and the separation module jointly. The proposed methodology is general enough to be applicable to a large class of neural network end-to-end separation systems. 
                        
                </div>
            </div>

         <div class="social-icons">
                <a class="social-icon" href="https://github.com/etzinis/two_step_mask_learning"><i class="fab fa-github"></i></a>
                 <a class="social-icon" href=""><i class="fab fa-telegram"></i></a>

            </div>   
            
        </div>
    </section>








    
    
 
    
        </div>
        
        </body>
</html>
