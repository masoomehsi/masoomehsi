<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
    <meta name="description" content=""/>
    <meta name="author" content=""/>
    <title> PASS  </title>
<!--    <link rel="icon" type="image/x-icon" href="./assets/img/favicon.ico"/>-->
    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet"
          type="text/css"/>
    <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css"/>
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="./css/styles.css" rel="stylesheet"/>
</head>

    <body id="page-top">
<!-- Navigation-->
<nav class="navbar navbar-expand-lg navbar-dark bg-secondary fixed-top" id="sideNav">
    <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none"> PASS (Persian Audio Source Separation) </span>
        <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-3"
                                             src="./assets/img/logo.JPG" alt=""/></span>
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span
            class="navbar-toggler-icon"></span></button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
            
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Mentor">Mentor Project</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Masoumehsiar">Speakers Count</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#FatemehJafari">Vocal-remover</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#mohammadi">Source separation 1</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Zargaran">Source separation 2</a></li>
            <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Reference">Reference</a></li>
        </ul> 
    </div>
</nav>
<!-- Page Content-->
<div class="container-fluid p-0">
    <!-- About-->
    <section class="resume-section" id="about">
        <div class="resume-section-content">
            <h1 class="mb-0">
                PASS
                <span class="text-info">Project</span>
            </h1>
            <div class="lead mb-5">
                Persian Audio Source Separation 
   
            </div>
            
            <div class="lead mb-5">

The Persian audio source separation project aims to separate two or more audio files from one audio containing two or more audio sources. Consider a audio file of a meeting in which four people are talking at the same time, and background music is playing; the final product of this project should be able to output five separate audio files, each containing one sound from one of them. Among these audio sources (4 people and a music source). The library in the references section is an open-source project ready for this work. This project may be enough for us (it may not depend on the language). Necessary measures should be taken to personalize it for the Persian language. After preliminary research and making a demo in the Persian language, it is time to test the product. It is placed as a research product in the field of sound processing. It is expected that an initial report will be posted on the company's blog after the research, and the community of this report and weekly reports will also make the project's final report.
                
                        </div>

            
            <div class="lead mb-1" style="color: #000000">
                Address: Sharif University of Technology, Azadi Ave, Tehran, Iran

            </div>
            <div class="lead mb-1">

                <a href="info@asr-gooyesh.com" style="color: #000000">Email: info@asr-gooyesh.com</a>

            </div>
            <div class="lead mb-5" style="color: #000000">

                Phone Number: 021-66551525
            </div>
            <div class="social-icons">
                <a class="social-icon" href="https://github.com/agp-internship/pass"><i class="fab fa-github"></i></a>
                 <a class="social-icon" href="https://www.linkedin.com/company/asr-gooyesh-pardaz-co-/?originalSubdomain=ir"><i class="fab fa-linkedin"></i></a>
                 <a class="social-icon" href="https://www.instagram.com/asrgooyesh"><i class="fab fa-instagram"></i></a>
                 <a class="social-icon" href="http://telegram.me/asrgooyeshpardaz"><i class="fab fa-telegram"></i></a>

            </div>
        </div>
           
        
        
        
        
        
        
        
        
        
        
        
        
        
         
        
        
 
        
         </section>

    <!-- Soroush Gooran -->
    <section class="resume-section" id="Mentor">
        <div class="resume-section-content">
            <h2 class="mb-5">Dr. Soroush Gooran</h2>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="./assets/img/DrGooran.png" style="width: 20%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="">
                    </a></h3>
                    <div> .</div>
                    <p class="mb-2">  </p>
                    <p>     About Mentor: 
   
                        PhD student in Artificial Intelligence at Sharif University of Technology  </p>
                    
                </div>
            </div>

 
            
        </div>
    </section>
    
    
    
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        </section>
    <hr class="m-0"/>
    
    <!-- Masoumeh siar-->
    <section class="resume-section" id="Masoumehsiar">
        <div class="resume-section-content">
            <h2 class="mb-5">Masoumeh siar</h2>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="./assets/img/masoumeh.png" style="width: 20%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="https://arxiv.org/pdf/1712.04555.pdf">Classification vs. Regression in Supervised Learning for Single Channel Speaker Count Estimation
                    </a></h3>
                    <div> In2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2018 Apr 15 (pp. 436-440). IEEE.</div>
                    <p class="mb-2">  </p>
                    <p> Abstract: The task of estimating the maximum number of concurrent speakers from single channel mixtures is important for various audio-based applications, such as blind source separation, speaker diarisation, audio surveillance or auditory scene classification. Building upon powerful machine learning methodology, we develop a Deep Neural Network (DNN) that estimates a speaker count. While DNNs efficiently map input representations to output targets, it remains unclear how to best handle the network output to infer integer source count estimates, as a discrete count estimate can either be tackled as a regression or a classification problem. In this paper, we investigate this important design decision and also address complementary parameter choices such as the input representation. We evaluate a stateof-the-art DNN audio model based on a Bi-directional Long Short-Term Memory network architecture for speaker count estimations. Through experimental evaluations aimed at identifying the best overall strategy for the task and show results for five seconds speech segments in mixtures of up to ten speakers.</p>
                    
                </div>
            </div>

         <div class="social-icons">
                <a class="social-icon" href="https://github.com/aishoot/Concurrent_Speakers_Counter"><i class="fab fa-github"></i></a>
                 <a class="social-icon" href=""><i class="fab fa-home"></i></a>
                 <a class="social-icon" href=""><i class="fab fa-telegram"></i></a>

            </div>   
            
        </div>
    </section>
    
    
    
    
    
    
    
    
    
    
 
    
    <!-- Fatemeh Jafari-->
    <section class="resume-section" id="FatemehJafari">
        <div class="resume-section-content">
            <h2 class="mb-5">Fatemeh Jafari</h2>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="./assets/img/jafari.jpg" style="width: 20%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="https://arxiv.org/pdf/1805.02410.pdf">MMDENSELSTM: AN EFFICIENT COMBINATION OF CONVOLUTIONAL AND
RECURRENT NEURAL NETWORKS FOR AUDIO SOURCE SEPARATION

                    </a></h3>
                    <div>  In2018 16th International workshop on acoustic signal enhancement (IWAENC) 2018 Sep 17 (pp. 106-110). IEEE.</div>
                    <p class="mb-2">  </p>
                    <p> Abstract: Deep neural networks have become an indispensable technique for audio source separation (ASS). It was recently reported that a variant of CNN architecture called MMDenseNet was successfully employed to solve the ASS problem of estimating source amplitudes, and state-of-the-art results were obtained for DSD100 dataset. To further enhance
MMDenseNet, here we propose a novel architecture that integrates long short-term memory (LSTM) in multiple scales with skip connections to efficiently model long-term structures within an audio context. The experimental results show
that the proposed method outperforms MMDenseNet, LSTM and a blend of the two networks. The number of parameters and processing time of the proposed model are significantly
less than those for simple blending. Furthermore, the proposed method yields better results than those obtained using ideal binary masks for a singing voice separation task.</p>
                    
                </div>
            </div>

         <div class="social-icons">
                <a class="social-icon" href="https://github.com/tsurumeso/vocal-remover"><i class="fab fa-github"></i></a>
                 <a class="social-icon" href=""><i class="fab fa-telegram"></i></a>

              </div>   
            
        </div>
    </section>
    
    
    
    
    
    
    
        
            </section>
    <hr class="m-0"/>
    <!-- mohammadi-->
    <section class="resume-section" id="mohammadi">
        <div class="resume-section-content">
            <h2 class="mb-5">‪Sajjad Mohammadi and AmirAli Rezaei</h2>  
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="./assets/img/mohammadi.png" style="width: 20%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="https://arxiv.org/pdf/2005.04132.pdf"> Asteroid: the PyTorch-based audio source separation toolkit for researchers

                    </a></h3>
                    <div>  arXiv preprint arXiv:2005.04132. 2020 May 8. </div>
                    <p class="mb-2">  </p>
                    <p> Abstract: This paper describes Asteroid, the PyTorch-based audio source separation toolkit for researchers. Inspired by the most successful neural source separation systems, it provides all neural building blocks required to build such a system. To improve reproducibility, Kaldi-style recipes on common audio source separation datasets are also provided. This paper describes the software architecture of Asteroid and its most important features. By showing experimental results obtained with Asteroid’s recipes, we show that our implementations are at least on par with most results reported in reference papers. The
toolkit is publicly available at github.com/mpariente/asteroid.
                        
                </div>
            </div>

         <div class="social-icons">
                <a class="social-icon" href="https://github.com/asteroid-team/asteroid"><i class="fab fa-github"></i></a>
                 <a class="social-icon" href=""><i class="fab fa-telegram"></i></a>

            </div>   
            
        </div>
    </section>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
   
    <!-- ‪Alireza Zargaran-->
    <section class="resume-section" id="Zargaran">
        <div class="resume-section-content">
            <h2 class="mb-5">‪Alireza Zargaran</h2>  
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="./assets/img/Zargaran.png" style="width: 20%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="https://arxiv.org/pdf/1910.09804.pdf"> TWO-STEP SOUND SOURCE SEPARATION: TRAINING ON LEARNED LATENT TARGETS
                    </a></h3>
                    <div>     InICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2020 May 4 (pp. 31-35). IEEE.
 </div>
                    <p class="mb-2">  </p>
                    <p> Abstract: In this paper, we propose a two-step training procedure for source separation via a deep neural network. In the first step we learn a transform (and it’s inverse) to a latent space where masking-based separation performance using oracles is optimal. For the second step, we train a separation module that operates on the previously learned space. In order to do so, we also make use of a scale-invariant signal to distortion ratio (SI-SDR) loss function that works in the latent space, and we prove that it lower-bounds the SI-SDR in the time domain. We run various sound separation experiments that show how
this approach can obtain better performance as compared to systems that learn the transform and the separation module jointly. The proposed methodology is general enough to be applicable to a large class of neural network end-to-end separation systems. 
                        
                </div>
            </div>

         <div class="social-icons">
                <a class="social-icon" href="https://github.com/etzinis/two_step_mask_learning"><i class="fab fa-github"></i></a>
                 <a class="social-icon" href=""><i class="fab fa-telegram"></i></a>

            </div>   
            
        </div>
    </section>







     
    
 
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
        </div>
    </section>
    <hr class="m-0"/>
    <!-- Reference-->
    <section class="resume-section" id="Reference">
        <div class="resume-section-content">
            <h2 class="mb-5">Reference</h2>
            <br>
            <Ul class="colorprimary" style="font-size: larger; padding-left: 0px; letter-spacing: 0.1em">
                <li>
Stöter FR, Chakrabarty S, Edler B, Habets EA. Classification vs. regression in supervised learning for single channel speaker count estimation. In2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2018 Apr 15 (pp. 436-440). IEEE.                </li>
                
                <li>
                    
              <p class="mb-3">  </p>

Takahashi N, Goswami N, Mitsufuji Y. Mmdenselstm: An efficient combination of convolutional and recurrent neural networks for audio source separation. In2018 16th International workshop on acoustic signal enhancement (IWAENC) 2018 Sep 17 (pp. 106-110). IEEE.
                </li>
                
              <p class="mb-3">  </p>

                <li>
Pariente M, Cornell S, Cosentino J, Sivasankaran S, Tzinis E, Heitkaemper J, Olvera M, Stöter FR, Hu M, Martín-Doñas JM, Ditter D. Asteroid: the PyTorch-based audio source separation toolkit for researchers. arXiv preprint arXiv:2005.04132. 2020 May 8.
                </li>
                
               <p class="mb-3">  </p>

                 <li>
Tzinis E, Venkataramani S, Wang Z, Subakan C, Smaragdis P. Two-step sound source separation: Training on learned latent targets. InICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2020 May 4 (pp. 31-35). IEEE.                     </li>
                
                <p class="mb-3">  </p>

           
             
                 </li>
                 </li>
                 </li>
     </li>
             </li>
            </Ul>
        </div>
    </section>
    <hr class="m-0"/>
</div>
<!-- Bootstrap core JS-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
<!-- Third party plugin JS-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
<!-- Core theme JS-->
<script src="./js/scripts.js"></script>
</body>
</html>

